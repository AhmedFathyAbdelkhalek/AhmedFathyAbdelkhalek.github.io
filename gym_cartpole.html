<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>AI-Powered Cartpole</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="/assets/css/main.css" />
		<noscript><link rel="stylesheet" href="/assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<!-- Note: The "styleN" class below should match that of the banner element. -->
					<header id="header" class="alt style2">
						<a href="index.html" class="logo"><strong>Home</strong></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
				<nav id="menu">
					<ul class="links">
						<li><a href="index.html">Home</a></li>
						<li><a href="profile.html">Profile</a></li>
						<li><a href="certifications.html">Certifications</a></li>
					</ul>
				</nav>	

				<!-- Banner -->
				<!-- Note: The "styleN" class below should match that of the header element. -->
					<section id="banner" class="style2">
						<div class="inner">
							<span class="image">
								<img src="/gifs/cartpole_small.gif" alt=""/>
							</span>
							<header class="major">
								<h1>AI-Powered Cartpole</h1>
							</header>
							<div class="content">
								<h3>This is a virtual environment simulating the cart-pole problem to showcase the power of reinforcement learning. </h3>
							</div>
						</div>
					</section>

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h2>Origin</h2>
									</header>
									<p>This project was part of my Reinforcement Learning and Optimal Control Course at the GUC and utilized the cart-pole environment from 
										<a href="https://www.gymlibrary.dev/environments/classic_control/cart_pole/" target="_blank">OpenAI Gym.</a> The course introduced
										finite MDPs, dynamic programming, and several reinforcement learning and optimal control techniques such as Q-Learning, Neural Networks,
										Actor-Critic, LQR, and MPC. This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in 
										<a href="https://ieeexplore.ieee.org/document/6313077" target="_blank">‚ÄúNeuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem‚Äù. </a>
									</p>
							</section>

						<!-- Two -->
							<section id="two" class="spotlights">
								<section>
									<a class="image center" href="https://aleksandarhaber.com/cart-pole-control-environment-in-openai-gym-gymnasium-introduction-to-openai-gym/" target="_blank">
										<img src="images/cartpole/env_description.jpg" alt="" data-position="center center" style="width:95%"/>
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Environment Description</h3>
											</header>
											<p>A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. 
												The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces 
												in the left and right direction on the cart. The observation space consists of the following states:
											</p>
											<div class="table-wrapper">
												<table class="alt">
														<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
														<thead>
															<tr>
																<th>State</th>
																<th>Symbol</th>
																<th>Range</th><th></th>
															</tr>
														</thead>
														<tbody>
															<tr>
																
																<td>Cart Position</td>
																<td>$$x$$</td>
																<td>[-4.8 , 4.8]</td>
															</tr>
															<tr>
																<td>Cart Velocity</td>
																<td>$$\dot{x}$$</td>
																<td>[-inf , inf]</td>
															</tr>
															<tr>
																<td>Pole Angle (deg)</td>
																<td>$$ ùúÉ$$</td>
																<td>[-24 , 24]</td>
															</tr>
															<tr>
																<td>Pole Angular Velocity</td>
																<td> $$ \dot{ùúÉ}$$</td>
																<td>[-inf , inf]</td>
															</tr>
														</tbody>
													</table>
												</div>
												<p>The episode terminates if the cart position exceeds |2.4| or the pole angle exceeds |12| degrees.</p>
										</div>
									</div>
								</section>
								<section>
									<a class="image center" href="https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial" target="_blank">
										<img src="images/cartpole/q_learning.png" alt="" data-position="center center" style="width:95%"/>
									</a>
									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Q-Learning</h3>
											</header>
											<ol> Q-learning is a model-free, value-based, off-policy algorithm that will find the best series of actions based on the agent's current state. 
												‚ÄúQ‚Äù stands for quality. Quality represents how valuable the action is in maximizing future rewards. Three distinct terms are present here
												<li> <b>Model-free:</b> the algorithm learns the consequences of its action through the experience without transition and reward function.</li>
												<li><b>Value-based:</b> the algorithms trains a value function to learn which state is most valuable and take action based on it.</li>
												<li><b>Off-policy:</b> the algorithm evaluates and updates a policy that differs from the policy used to take an action.</li>
											</ol>
											<ul> <b>The aim is to create a Q-Learning agent that</b>
												<li>Uses an ùúñ-greedy policy for exploration with ùúñ = 0.05.</li>
												<li>Uses a learning rate ùõº = 0.1</li>
												<li>Uses a discount factor ùõæ = 0.9.</li>
											</ul>
										</div>
									</div>
								</section>
								<section>
									<iframe width="560" height="575" src="https://www.youtube.com/embed/gD01bGGAdZs?si=GcBPTu4H8qsGd0cA" title="YouTube video player" 
									frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; 
									web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

									<script>
										document.getElementById('vid').play();
									</script>

									<div class="content">
										<div class="inner">
											<header class="major">
												<h3>Results</h3>
											</header>
											<p>It is clear that the agent gets a larger average reward after each episode, as seen in the figure below. After 5000 
												training episodes, the agent gets an average reward of 120.39. The video shows that the trained agent can 
												balance the cart for as long as possible until the episode terminates.
												<div style="text-align: center;">
													<img src="images/cartpole/mean_reward.png" alt="" style="width:90%;"/> 
												</div>
											</p>
										</div>
									</div>
								</section>
							</section>

					</div>

				<!-- Contact -->
				<section id="contact">
					<div class="inner">
						<section class="split">
							<section>
								<div class="contact-method">
									<span class="icon solid alt fa-envelope"></span>
									<h3>Email</h3>
									<a href="mailto:ahmedfathyabdelkhalek@gmail.com" target="_blank">ahmedfathyabdelkhalek@gmail.com</a>
								</div>
							</section>
						</section>
					</div>
				</section>

			<!-- Footer -->
				<footer id="footer">
					<div class="inner">
						<ul class="icons">
							<li><a href="https://github.com/AhmedFathyAbdelkhalek" target="_blank" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://www.linkedin.com/in/ahmed-fathy-abdelkhalek/" target="_blank" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://www.researchgate.net/profile/Ahmed-Fathy-91" target="_blank" class="icon brands alt fa-researchgate"><span class="label">ResearchGate</span></a></li>
						</ul>
						<ul class="copyright">
							<li>&copy; CCA 3.0</li>
						</ul>
					</div>
				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>